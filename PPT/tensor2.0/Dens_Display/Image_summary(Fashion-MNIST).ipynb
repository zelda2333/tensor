{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import io\n",
    "import itertools\n",
    "from six.moves import range\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the data. The data is already divided into train and test.\n",
    "# The labels are integers representing classes.\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) =fashion_mnist.load_data()\n",
    "\n",
    "# Names of the integer classes, i.e., 0 -> T-short/top, 1 -> Trouser, etc.\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear out prior logging data.\n",
    "!rm -rf logs/plots\n",
    "\n",
    "logdir = \"logs/plots/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "def plot_to_image(figure):\n",
    "  \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "  returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "  # Save the plot to a PNG in memory.\n",
    "  buf = io.BytesIO()\n",
    "  plt.savefig(buf, format='png')\n",
    "  # Closing the figure prevents it from being displayed directly inside\n",
    "  # the notebook.\n",
    "  plt.close(figure)\n",
    "  buf.seek(0)\n",
    "  # Convert PNG buffer to TF image\n",
    "  image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "  # Add the batch dimension\n",
    "  image = tf.expand_dims(image, 0)\n",
    "  return image\n",
    "\n",
    "def image_grid():\n",
    "  \"\"\"Return a 5x5 grid of the MNIST images as a matplotlib figure.\"\"\"\n",
    "  # Create a figure to contain the plot.\n",
    "  figure = plt.figure(figsize=(10,10))\n",
    "  for i in range(25):\n",
    "    # Start next subplot.\n",
    "    plt.subplot(5, 5, i + 1, title=class_names[train_labels[i]])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "  \n",
    "  return figure\n",
    "\n",
    "# Prepare the plot\n",
    "figure = image_grid()\n",
    "# Convert to image and log\n",
    "with file_writer.as_default():\n",
    "  tf.summary.image(\"Training data\", plot_to_image(figure), step=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "  \"\"\"\n",
    "  Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "  Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "  \"\"\"\n",
    "  figure = plt.figure(figsize=(8, 8))\n",
    "  plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "  plt.title(\"Confusion matrix\")\n",
    "  plt.colorbar()\n",
    "  tick_marks = np.arange(len(class_names))\n",
    "  plt.xticks(tick_marks, class_names, rotation=45)\n",
    "  plt.yticks(tick_marks, class_names)\n",
    "\n",
    "  # Normalize the confusion matrix.\n",
    "  cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "  # Use white text if squares are dark; otherwise black.\n",
    "  threshold = cm.max() / 2.\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "    plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel('True label')\n",
    "  plt.xlabel('Predicted label')\n",
    "  return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_confusion_matrix(epoch, logs):\n",
    "  # Use the model to predict the values from the validation dataset.\n",
    "  test_pred_raw = model.predict(test_images)\n",
    "  test_pred = np.argmax(test_pred_raw, axis=1)\n",
    "\n",
    "  # Calculate the confusion matrix.\n",
    "  cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)\n",
    "  # Log the confusion matrix as an image summary.\n",
    "  figure = plot_confusion_matrix(cm, class_names=class_names)\n",
    "  cm_image = plot_to_image(figure)\n",
    "\n",
    "  # Log the confusion matrix as an image summary.\n",
    "  with file_writer_cm.as_default():\n",
    "    tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n",
    "\n",
    "# Define the per-epoch callback.\n",
    "cm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0713 03:27:41.375843 140176756545280 deprecation.py:323] From /home/zelda/anaconda3/envs/tensor/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 13s 221us/sample - loss: 0.5535 - accuracy: 0.8115 - val_loss: 0.4622 - val_accuracy: 0.8353\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 12s 198us/sample - loss: 0.4064 - accuracy: 0.8569 - val_loss: 0.4182 - val_accuracy: 0.8528\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 12s 198us/sample - loss: 0.3746 - accuracy: 0.8668 - val_loss: 0.3935 - val_accuracy: 0.8623\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 12s 198us/sample - loss: 0.3552 - accuracy: 0.8716 - val_loss: 0.3910 - val_accuracy: 0.8605\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.3405 - accuracy: 0.8766 - val_loss: 0.3837 - val_accuracy: 0.8621\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 12s 203us/sample - loss: 0.3301 - accuracy: 0.8791 - val_loss: 0.3991 - val_accuracy: 0.8595\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 12s 193us/sample - loss: 0.3192 - accuracy: 0.8844 - val_loss: 0.3692 - val_accuracy: 0.8683\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 12s 196us/sample - loss: 0.3108 - accuracy: 0.8862 - val_loss: 0.3818 - val_accuracy: 0.8676\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 16s 269us/sample - loss: 0.3036 - accuracy: 0.8878 - val_loss: 0.3708 - val_accuracy: 0.8668\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 17s 291us/sample - loss: 0.2958 - accuracy: 0.8910 - val_loss: 0.3750 - val_accuracy: 0.8648\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 18s 308us/sample - loss: 0.2900 - accuracy: 0.8928 - val_loss: 0.3602 - val_accuracy: 0.8708\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 19s 318us/sample - loss: 0.2849 - accuracy: 0.8957 - val_loss: 0.3701 - val_accuracy: 0.8708\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 16s 262us/sample - loss: 0.2802 - accuracy: 0.8975 - val_loss: 0.3583 - val_accuracy: 0.8734\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 19s 313us/sample - loss: 0.2743 - accuracy: 0.8987 - val_loss: 0.3592 - val_accuracy: 0.8722\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 15s 246us/sample - loss: 0.2699 - accuracy: 0.9008 - val_loss: 0.3667 - val_accuracy: 0.8747\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 14s 240us/sample - loss: 0.2659 - accuracy: 0.9017 - val_loss: 0.3673 - val_accuracy: 0.8702\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 14s 230us/sample - loss: 0.2617 - accuracy: 0.9031 - val_loss: 0.3699 - val_accuracy: 0.8682\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 15s 249us/sample - loss: 0.2594 - accuracy: 0.9042 - val_loss: 0.3587 - val_accuracy: 0.8722\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 14s 238us/sample - loss: 0.2563 - accuracy: 0.9038 - val_loss: 0.3598 - val_accuracy: 0.8756\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 15s 246us/sample - loss: 0.2538 - accuracy: 0.9058 - val_loss: 0.3601 - val_accuracy: 0.8742\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 13s 213us/sample - loss: 0.2510 - accuracy: 0.9068 - val_loss: 0.3667 - val_accuracy: 0.8694\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 13s 210us/sample - loss: 0.2470 - accuracy: 0.9079 - val_loss: 0.3836 - val_accuracy: 0.8677\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 15s 244us/sample - loss: 0.2448 - accuracy: 0.9090 - val_loss: 0.3907 - val_accuracy: 0.8661\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 14s 227us/sample - loss: 0.2422 - accuracy: 0.9098 - val_loss: 0.3743 - val_accuracy: 0.8751\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 13s 214us/sample - loss: 0.2401 - accuracy: 0.9113 - val_loss: 0.3804 - val_accuracy: 0.8715\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 14s 230us/sample - loss: 0.2362 - accuracy: 0.9126 - val_loss: 0.3918 - val_accuracy: 0.8646\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 14s 234us/sample - loss: 0.2360 - accuracy: 0.9118 - val_loss: 0.3776 - val_accuracy: 0.8695\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 13s 215us/sample - loss: 0.2326 - accuracy: 0.9150 - val_loss: 0.3816 - val_accuracy: 0.8710\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 14s 237us/sample - loss: 0.2304 - accuracy: 0.9147 - val_loss: 0.3774 - val_accuracy: 0.8702\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 14s 229us/sample - loss: 0.2280 - accuracy: 0.9175 - val_loss: 0.3734 - val_accuracy: 0.8729\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 13s 217us/sample - loss: 0.2247 - accuracy: 0.9174 - val_loss: 0.3901 - val_accuracy: 0.8715\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 13s 214us/sample - loss: 0.2253 - accuracy: 0.9160 - val_loss: 0.3907 - val_accuracy: 0.8706\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 12s 208us/sample - loss: 0.2227 - accuracy: 0.9168 - val_loss: 0.3839 - val_accuracy: 0.8705\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 14s 238us/sample - loss: 0.2195 - accuracy: 0.9188 - val_loss: 0.3845 - val_accuracy: 0.8702\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 21s 343us/sample - loss: 0.2189 - accuracy: 0.9184 - val_loss: 0.4041 - val_accuracy: 0.8630\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 22s 362us/sample - loss: 0.2167 - accuracy: 0.9187 - val_loss: 0.4113 - val_accuracy: 0.8662\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 23s 391us/sample - loss: 0.2130 - accuracy: 0.9212 - val_loss: 0.4183 - val_accuracy: 0.8678\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 22s 364us/sample - loss: 0.2148 - accuracy: 0.9197 - val_loss: 0.3876 - val_accuracy: 0.8730\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 20s 342us/sample - loss: 0.2121 - accuracy: 0.9211 - val_loss: 0.3951 - val_accuracy: 0.8703\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 19s 317us/sample - loss: 0.2101 - accuracy: 0.9219 - val_loss: 0.4070 - val_accuracy: 0.8656\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 21s 345us/sample - loss: 0.2093 - accuracy: 0.9230 - val_loss: 0.3975 - val_accuracy: 0.8736\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 21s 351us/sample - loss: 0.2070 - accuracy: 0.9228 - val_loss: 0.4050 - val_accuracy: 0.8744\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 23s 381us/sample - loss: 0.2061 - accuracy: 0.9233 - val_loss: 0.4162 - val_accuracy: 0.8700\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 24s 394us/sample - loss: 0.2034 - accuracy: 0.9244 - val_loss: 0.4038 - val_accuracy: 0.8734\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 25s 415us/sample - loss: 0.2043 - accuracy: 0.9237 - val_loss: 0.4052 - val_accuracy: 0.8701\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 22s 368us/sample - loss: 0.2009 - accuracy: 0.9254 - val_loss: 0.4109 - val_accuracy: 0.8720\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 21s 349us/sample - loss: 0.2000 - accuracy: 0.9256 - val_loss: 0.4103 - val_accuracy: 0.8722\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 20s 335us/sample - loss: 0.1978 - accuracy: 0.9267 - val_loss: 0.4287 - val_accuracy: 0.8638\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 19s 316us/sample - loss: 0.1981 - accuracy: 0.9264 - val_loss: 0.4192 - val_accuracy: 0.8717\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 20s 330us/sample - loss: 0.1943 - accuracy: 0.9287 - val_loss: 0.4222 - val_accuracy: 0.8727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7cfd0ee278>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier.\n",
    "model.fit(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    epochs=50,\n",
    "    #verbose=0, # Suppress chatty output\n",
    "    callbacks=[tensorboard_callback, cm_callback],\n",
    "    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 114us/sample - loss: 0.4222 - accuracy: 0.8727\n",
      "Test accuracy: 0.8727\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
